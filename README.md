## SEGMENTAÃ‡ÃƒO DE IMAGENS ##

Esse trabalho consiste na segmentaÃ§Ã£o semÃ¢ntica de imagens de OCT (Optical Coherence Tomography)
com o intuito de facilitar a detecÃ§Ã£o das camadas que compÃµem a retina e as doenÃ§as que acometem
essa Ã¡rea ocular.

# 1 - dataset usado
o dataset consiste num conjunto de dados de 10 pacientes arquivados em documentos .mat. Esses ar-
quivos basicamente sÃ£o dicionÃ¡rios python, em que contÃ©m informaÃ§Ãµes dos pacientes, como suas imagens
e as anotaÃ§Ãµes que os mÃ©dicos especialistas fizeram.

    * cada paciente possui 61 B-scans, mas nem todos possuem anotaÃ§Ã£o
    * inicialmente estÃ¡ sendo analisadas as anotaÃ§Ãµes da chave "manualLayers1"

# 2 - preparaÃ§Ã£o de imagens
Por meio dos arquivos .mat foram filtradas as imagens de cada paciente que possuem anotaÃ§Ã£o por meio
de uma classe definida em que apÃ³s localizar essas imagens irÃ¡ definir :

    * images - Ã© criado um diretÃ³rio para armazenar as imagens que possuem anotaÃ§Ãµes jÃ¡ cortadas
                            |
              |__manualLayers1 - imagens usadas para criar mascaras de acordo com o especialista 1
              
              |
              |__manualLayers2 - imagens usadas para criar mascaras de acordo com o especialista 2
              
              |
              |__manualFluid1 -  imagens usadas para criar mascaras dos fluidos de acordo com o especialista 1
              
              |
              |__manualFluid2 - imagens usadas para criar mascaras dos fluidos de acordo com o especialista 2
    
    * masks 
              |
              |__manualLayers1 - mÃ¡scaras das camadas definidas pelo especialista 1
              
              |
              |__manualLayers2 - mÃ¡scaras das camadas definidas pelo especialista 2
              
              |
              |__manualFluid1 -  mÃ¡scaras dos flÃºidos definidas pelo especialista 1
              
              |
              |__manualFluid2 - mÃ¡scaras dos flÃºidos definidas pelo especialista 2

Uma mesma imagem possui as 4 anotaÃ§Ãµes menciondas, ou seja, nÃ£o hÃ¡ diferentes imagens para cada
tipo de mÃ¡scara mencionado.


# 3 - treinamento da rede neural

# info
Esse este estudo se baseia no artigo https://opg.optica.org/boe/fulltext.cfm?uri=boe-6-4-1172&id=312754 que busca identificar em imagens de  OCT ÃREAS DE FLUÃDOS e SETE CAMADAS DA RETINA COM
DME (Diabetic Macular Edema). Esse artigo propÃµe um algoritmo que busca prever a posiÃ§Ã£o dessas regiÃµes nas imagens. Fizeram isso em 110 B-scans de 10 pacientes com DME severa com um ALGORITMO BASEADO EM REGRESSÃƒO DE KERNEL

Layers
        |
        |__ILM : Inner Limiting Membrane
        |
        |__NFL/GCl : Nerve Fiber Layer 
        |
        |__IPL/INL : Inner Plexiform layer e Inner Nuclear Layer
        |
        |__OPL/ONL : Outer Plexiform Layer e Outer Nuclear Layer
        |
        |__ISM/ISE : Inner Segment Myeloid e Inner Segment Ellipsoid
        |
        |__OS/RPE : Outer Segment e Retinal Pigment Epithelium
        |
        |__BM : Bruch's Membrane

Dois oftalmologistas segmentaram manualmente todas as regiÃµes cheias de lÃ­quido e oito limites da camada retinal usando software personalizado (DOCTRAP V50.9) para 110 imagens (11 por paciente).

=============================================================================================
# Structured layer surface segmentation for retina OCT using fully convolutional regression networks

camadas da retina sÃ£o importantes biomarcadores para anomalias na retina como edema macular diabÃ©tico (DME)
ou anomalias neurolÃ³gicas como Esclerose MÃºltipla (EM). E a segmentaÃ§Ã£o manual dessas imagens de OCT sÃ£o de-
moradas.


- Roy et al. (2017) usaram uma rede totalmente convolucional (FCN) para rotular cada pixel em oito classes de camadas, edema e fundo

- Lee et al. (2017a) e Schlegl et al. (2018) cada um usou um FCN para segmentar edema e fluidos retinianos

- Venhuizen et al. (2017) usado um FCN para segmentar toda a retina sem segmentar cada camada

- O segundo tipo de classificador rotula pixels como superfÃ­cies (ou seja,limites entre as camadas da 
retina ou entre a retina e seus
antecedentes)

retinal layers:

    |_the retinal nerve fiber layer (RNFL);
    |
    |_the ganglion cell layer (GCL) combined with the inner plexiform layer (IPL), denoted as GCIP;
    |
    |_the inner nuclear layer (INL); the outer plexiform layer (OPL);
    |
    |_the outer nuclear layer (ONL), the inner segment (IS);
    |
    |_the outer segment (OS);
    |
    |_and the retinal pigment epithelium (RPE);
    |
    |_Surfaces between these layers are identified by hyphenating
    |
    |_their acronyms. The other named surfaces are: the inner limiting membrane (ILM);
    |
    |_the external limiting membrane (ELM); --> nÃ£o tem no dataset da IC
    |
    |_and Bruchâ€™s Membrane (BM).
    |
    |_Finally, above the RNFL is the vitreous and below the BM is the choroid.

Nossa rede tem o benefÃ­cio de: 
1) ser treinado de ponta a ponta;

2) melhorar a precisÃ£o contra o estado da arte;

3) ser leve porque nÃ£o usa uma camada totalmente conectada para regressÃ£o. TambÃ©m realizamos uma anÃ¡lise da inclinaÃ§Ã£o da superfÃ­cie para mostrar que a conectividade da superfÃ­cie estÃ¡ bem restrita, mesmo sem restriÃ§Ãµes explÃ­citas, como no caso mÃ©todos grÃ¡ficos.

A rede construÃ­da tem dois ramos de saÃ­da :

    * o primeiro gera uma segmentaÃ§Ã£o de camadas com rotulagem em pixels e lesÃµes

    * o segundo modelo a distribuiÃ§Ã£o das posiÃ§Ãµes da superfÃ­cie e produz as posiÃ§Ãµes
      de cada superfÃ­cie em cada coluna (A-scan)

    Ambos os ramos compartilham o mesmo extrator de caracterÃ­sticas que Ã© U-Net residual.
    O input Ã© uma imagem de tres canais

======================================
MÃ©todo de avaliaÃ§Ã£o do modelo proposto
======================================

O esquema de rotulagem por pixel tem problemas topolÃ³gicos com camadas biologicamente estruturadas. No entanto, lesÃµes (lÃ­quido ou edema) que aparecem em locais diferentes e
com formas diferentes nÃ£o podem receber uma topologia fixa e, portanto, a rotulagem em 
pixels Ã© apropriada.

usaram dois datasets: t (He et al., 2019c) contains 14 healthy controls (HC) and 21 people with MS (PwMS) e o (Chiu et al., 2015) contains 110 Bscans (496 Ã— 768) from 10 diabetic macular edema (DME) patients.

No segundo dataset (usado na IC), os 5 primeiros pacientes possuem DME severa com sequelas
na estrutura retiniana. Foram delineadas 8 camadas e os edemas encontrados. A divisÃ£o foi
50% - 50% treino/teste segundo (Chiu et al., 2015; Rathke et al., 2017; Karri et al., 2016; Roy et al., 2017). Treinaram nas 55 Ãºltimas imagens atÃ© o treinamento convergir e foi testada
nas primeiras 55 imagens que apresentam edema gravÃ­ssimo.

- as imagens foram achatadas e cortadas para 224 x 768 e usaram pytorch
- hiperparÃ¢metros: Adam optmizer; learning rate: 10e-4; weight decay: 10e-4; minibatch size: 2
- aplicou-se data augmentation com horizontal flipping e vertical scaling, ambos com probabilidade 0.5, e a imagem em escala foi cortada no mesmo tamanho anttes de dimensionar

No DME Datasets fizeram uma comparaÃ§Ã£o de resultados. Mediram o Dice Score de
Chiu et al. (2015), ReLayNet e o modelo proposto que alcanÃ§aram os valores 0.56,
0.70 e 0.70 respectivamente

==============================================================================================
# OCT layers segmetation using U-NET semantic segmentation and RESNET34 encoder-decoder

accuracy e mean Intersection over Union (mIoU) foram mÃ©tricas utilizadas levando em consideraÃ§Ã£o dois benchmarks: DeepLabV3Plus e UnetP++. Foi usado Pytorch e python 3.9

* Wenzhou Medical University (WMU) forneceu o dataset

encontrou-se 0.92, 0.90 e 0.95 de acurÃ¡cia, sensibilidade e especificidade respectivamente.

600 imagens de OCT que procedem de 40 olhos e 15 imagens selecionadas para o primeiro dataset

mIoU = pontuaÃ§Ã£o mÃ©dia da IoU em todas as classes ou conjunto de dados


==============================================================================================
A avaliaÃ§Ã£o de modelos de segmentaÃ§Ã£o semÃ¢ntica deve levar em consideraÃ§Ã£o se os dados sÃ£o balanceados ou nÃ£o, pois a irregularidade de quantidades de pixels para cada classe pode
mascarar os resultados, o que leva a mÃ©trica de acurÃ¡cia nÃ£o ser muito eficiente na avaliaÃ§Ã£o

* DICE LOSS

    essa mÃ©trica avalia a dissimilaridade entre a segmentaÃ§Ã£o predita e a segmentaÃ§Ã£o 
    verdadeira. Isso Ã© uma variaÃ§Ã£o de Dice Similarity Coefficient conhecido como
    SÃ¸rensenâ€“Dice coefficient, mÃ©trica que compara a similaridade entre dois samples.
    A Dice Loss compara a similaridade de duas classificaÃ§Ãµes binÃ¡rias da segmentaÃ§Ã£o
    verdadeira e da segmentaÃ§Ã£o predita. O objetivo Ã© diminuir a diferenÃ§a entre as
    segmentaÃ§Ãµes, a qual Ã© chamada de funÃ§Ã£o de perda.

        DiceLoss(ğ‘¦,ğ‘Ì… ) = 1âˆ’(2ğ‘¦ğ‘Ì… +1)/(ğ‘¦+ğ‘Ì…+1)

        SÃ¸rensenâ€“Dice coefficient(ğ‘¦,ğ‘Ì…) = 2*(ğ‘¦ âˆ© ğ‘Ì…)/(ğ‘¦ + ğ‘Ì…)

        Dice Loss = 1 - Dice Coefficient

        ğ‘¦ - segmentaÃ§Ã£o verdadeira
        ğ‘Ì… - segmentaÃ§Ã£o predita

    ela gera um valor entre 0 e 1, sendo 0 dissimilaridade e 1 similaridade completa entre
    as segmentaÃ§Ã£o. Ã‰ o oposto do Dice Similarity Coefficient.


*******************************************************************************************
* smooth = 10e-3                                                                          *
* def dice_coef(y_true, y_pred):                                                          *
*    y_true_f = K.flatten(y_true)                                                         *
*    y_pred_f = K.flatten(y_pred)                                                         *
*    intersection = K.sum(y_true_f * y_pred_f)                                            *
*    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)   *
*******************************************************************************************


* IoU (INTERSECTION OVER UNION - JACCARD INDEX) 

    IoU = (ğ‘¦ âˆ© ğ‘Ì…)/(ğ‘¦ âˆª ğ‘Ì…)
    IoU Loss = 1 - Iou

**********************************************************************************************
* def mIOU(prediction, label, num_classes):                                                  *
*    prediction= prediction.max(1)[1].float().cpu().numpy()                                  *
*    label = label.float().cpu().numpy()                                                     *
*    iou_list = list()                                                                       *
*    present_iou_list = list()                                                               *
*                                                                                            *
*    for sem_class in range(num_classes):                                                    *
*        pred_inds = (pred == sem_class)                                                     *
*        target_inds = (label == sem_class)                                                  *
*        if target_inds.sum().item() == 0:                                                   *
*            iou_now = float('nan')                                                          *
*        else:                                                                               *
*            intersection_now = (pred_inds[target_inds]).sum().item()                        *
*            union_now = pred_inds.sum().item() + target_inds.sum().item() - intersection_now*
*            iou_now = float(intersection_now) / float(union_now)                            *
*            present_iou_list.append(iou_now)                                                *
*        iou_list.append(iou_now)                                                            *
*    miou = np.mean(present_iou_list)                                                        *
*    return miou                                                                             *
**********************************************************************************************n